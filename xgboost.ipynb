{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import random\n",
    "import datetime as dt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "features = pd.read_csv('data/train_features.csv')\n",
    "labels = pd.read_csv('data/train_label.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Preproccessing </h3>\n",
    "\n",
    "Preprocess our input to make it compatible with XGBoost. We need to replace NaN's \n",
    "and encode categorical features. Since there are multiple classes for most of them d\n",
    "dummy variables would exponentially increase the feature space, we use label encoding instead.\n",
    "\n",
    "We also engineer some features, such as the month extraction from the transaction date field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def drop_unchecked(df, cols):   \n",
    "    \"\"\"\n",
    "    An unchecked version of pandas.DataFrame.drop(cols, axis=1). This will not raise \n",
    "    an error in case of non existing column. Be careful though as this might hide spelling errors\n",
    "    \"\"\"\n",
    "    for col in (set(cols) & set(df.columns)):\n",
    "        df = df.drop([col], axis=1)\n",
    "    return df\n",
    "\n",
    "# Enable OOP usage: df.drop_unchecked(cols) instead of drop_unchecked(df, cols)\n",
    "pd.DataFrame.drop_unchecked = drop_unchecked\n",
    "\n",
    "for c in features.columns:\n",
    "    # Replace NaNs\n",
    "    features[c] = features[c].fillna(-1)\n",
    "    if features[c].dtype == 'object':\n",
    "        # Encode categorical features\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(features[c].values))\n",
    "        features[c] = lbl.transform(list(features[c].values))\n",
    "\n",
    "# Drop some useless or extremely rare features\n",
    "features = features.drop_unchecked(['propertyzoningdesc', 'propertycountylandusecode',\n",
    "                                    'fireplacecnt', 'fireplaceflag'])\n",
    "\n",
    "# Create additional features. Month will be used to split into training and validation\n",
    "labels[\"transactiondate\"] = pd.to_datetime(labels[\"transactiondate\"])\n",
    "labels[\"Month\"] = labels[\"transactiondate\"].dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Now we are ready to merge out features with their labels. We only keep properties for which the label is known **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = labels.merge(features, how='left', on='parcelid')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Split the dataset into training and validation </h3>\n",
    "\n",
    "\n",
    "In order to reliably evaluate a model we need to eliminate the effect of overfitting. That is achieved by\n",
    "evaluating our model on data that were not used during training. For this reason we split the data into\n",
    "training (first 9 months of transactions) and validation (last 3 months). This is roughly equivalent to a 75 - 25 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ID and date are no longer usefull\n",
    "train = train.drop_unchecked(['parcelid', 'transactiondate'])\n",
    "\n",
    "# Split into training and validation sets\n",
    "x_train = train[train[\"Month\"] < 10]                       \n",
    "x_val   = train[train[\"Month\"] >= 10]  \n",
    "\n",
    "# Drop some outliers. This is debatable; arguably records with extraordinary log error are the most interesting\n",
    "x_train = x_train.query('logerror > -0.4 and logerror < 0.4')\n",
    "\n",
    "y_train = x_train['logerror'].values\n",
    "y_val = x_val['logerror'].values\n",
    "\n",
    "x_train = x_train.drop_unchecked(['logerror'])\n",
    "x_val = x_val.drop_unchecked(['logerror'])\n",
    "   \n",
    "print('Shape train: {}\\nShape validation: {}'.format(x_train.shape, x_val.shape))       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Lets train the model! </h3>\n",
    "\n",
    "Our input is now ready to train a model! The only thing missing is tuning the models hyper-parameters.\n",
    "We start by reusing parameters open sourced on kaggle - we should later use a more thorough approach such as gridsearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Our base prediction will be the number optimizing for MAE, that is the median\n",
    "y_mean = np.median(y_train)\n",
    "\n",
    "##### RUN XGBOOST\n",
    "print(\"\\nSetting up data for XGBoost ...\")\n",
    "# xgboost params\n",
    "xgb_params = {\n",
    "    'eta': 0.037,\n",
    "    'max_depth': 5,\n",
    "    'subsample': 0.80,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'mae',\n",
    "    'lambda': 0.8,   \n",
    "    'alpha': 0.4, \n",
    "    'base_score': y_mean,\n",
    "    'silent': 1\n",
    "}\n",
    "num_boost_rounds = 500\n",
    "\n",
    "dtrain = xgb.DMatrix(x_train, y_train)\n",
    "dtest = xgb.DMatrix(x_val)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train the model. Expect your machine to overheat here.\n",
    "print( \"\\nTraining XGBoost ...\")\n",
    "model = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)\n",
    "\n",
    "print( \"\\nPredicting with XGBoost ...\")\n",
    "prediction = model.predict(dtest)\n",
    "\n",
    "mae = mean_absolute_error(y_val, prediction)\n",
    "\n",
    "print(\"\\n##########\")\n",
    "print(\"Mean Absolute Error is: \", mae)\n",
    "print(\"##########\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> These results look promising! </h3>\n",
    "\n",
    "On the first try we achieve a **MAE of 0.06491**. This is already good enough to put us in the top 50% of competitors. It is obvious that XGBoost has a lot to offer, careful tuning and ensembling should probably be our next steps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
