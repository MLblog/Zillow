{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Eploring the Zillow prize dataset </h1>\n",
    "\n",
    "In this notebook we are going to explore the provided datasets in order to better understand the problem at hand. At the same time we will try to arrive at preliminary conclusions on the distribution of our data in order to derive useful insights for the modelling phase.\n",
    "\n",
    "Lets start by loading the training dataset for inspection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # Bad practice my ass\n",
    "\n",
    "train_features = pd.read_csv('data/train_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Renaming the columns </h3>\n",
    "\n",
    "It is obvious that these column names are hard to interpret, we will therefore use a custom mapping. Since\n",
    "this falls more into the data layer than the analysis we will read the mapping from a file. The expected format\n",
    "is a key value pair per line split by the \"=\" symbol. \n",
    "\n",
    "For example: \n",
    "\n",
    "`newColumnName = oldColumnName`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_mapping = pd.read_csv('data/feature_names', sep =\"=\", header=None).applymap(str.strip)\n",
    "mapping_dict = dict(zip(col_mapping[1], col_mapping[0]))\n",
    "train_features.rename(columns = mapping_dict, inplace=True)\n",
    "\n",
    "(num_records, num_features) = train_features.shape\n",
    "print('There are {0} properties recorded and {1} features in total'.format(*train_features.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Handling sparse columns </h3>\n",
    "\n",
    "There are quite a lot of features, however some of them have a lot of missing values. Lets try to quantify that statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nan_count = train_features.isnull().mean()\n",
    "\n",
    "threshold = 0.99\n",
    "print('Some features with more than {0}% NaN values: \\n'.format(threshold * 100))\n",
    "print(nan_count[nan_count > threshold].sort_values(ascending=False))\n",
    "\n",
    "plot = sns.distplot(nan_count * 100, hist=False, rug=True)\n",
    "plot.set(xlim=(0, 100), yticks=[], ylabel='Features', xlabel='NaN %')\n",
    "sns.plt.title('NaN Percentage', weight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> More than 1/3 of our features are sparse! </h4>\n",
    "\n",
    "Several of these sparse features could be important such as the tax delinquency. In others however a missing value can be easily imputed. For example missing values for the pool type and area probably mean that the property does not have a pool installed. Moreover the histogram seems inverserly Guassian, this means that features are either mainly blank (right side of the curve) or almost always filled (left side).\n",
    "\n",
    "<i>These missing values may or may not pose a significant problem at the modelling phase</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3> Checking the target variable </h3>\n",
    "\n",
    "Lets now take a look at our target variable: <b> The log Error </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = pd.read_csv('data/train_label.csv')\n",
    "\n",
    "# Rename is needed to facilitate a left join\n",
    "labels.rename(columns={'parcelid': 'ID'}, inplace=True)\n",
    "\n",
    "# What part of properties have an associated label?\n",
    "print('Only ' + '{0:.2f}'.format(len(labels) * 100 / len(train_features)) + '% properties are labeled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> A lot of missing labels </h3>\n",
    "\n",
    "Well it seems that most of the properties found in our feature dataset do not have a label associated with them.\n",
    "This could probably come from the fact that a prediction error can only be computed when the real selling price is recorded, that is when the property is actually sold. Of course not every property was sold within the time limits of the data collection. This will significantly reduce our training set since we will only keep records where the label is known for the modelling phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge the 2 datasets, only keeping properties for which the target variable is known\n",
    "merged = train_features.merge(labels, on='ID', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Univariate analysis </h3>\n",
    "\n",
    "Lets start by identifying features that are correlated to the target variable. Note that this only shows \n",
    "linear relations between each feature and the target variable (and not combinations of features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_corr(feature):\n",
    "    target = merged['logerror']\n",
    "    if feature.isnull().mean() > 0.2:\n",
    "        # Too many NaN, imputing does not make sense\n",
    "        return None\n",
    "    try:\n",
    "        imputed_feature = feature.fillna(feature.mean())\n",
    "        return np.corrcoef(imputed_feature, target)[0, 1]\n",
    "    except TypeError:\n",
    "        # Cannot correlate categorical features\n",
    "        return None\n",
    "    \n",
    "# Lets find the cross correlation of every numerical feature to the target variable\n",
    "coeffs = merged.drop(['ID', 'logerror'], axis=1).apply(cross_corr)\n",
    "print('These features show the highest correlation (in absolute value) to the target variable: \\n')\n",
    "print(coeffs.dropna().abs().sort_values(ascending=False).head(5))\n",
    "\n",
    "# Lets plot for easier inspection\n",
    "univariate = pd.DataFrame({'features': coeffs.index, 'correlation coef.': coeffs.values})\n",
    "plot = sns.barplot('features', 'correlation coef.', data=univariate)\n",
    "plot.set(ylim=(coeffs.min(), coeffs.max()), ylabel='pearson coeff.', title='Univariate Correlation')\n",
    "plot.set_xticklabels(coeffs.index, rotation=90)#if this is confusing comment it out\n",
    "sns.plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Univariate analysis not too promising </h3>\n",
    "\n",
    "Observations:\n",
    "- Only a subset of our features have a meaningful correlation to the target value (for example categorical one's or numerical with many missing values do not).\n",
    "\n",
    "- Most features are positively correlated to he target (higher values lead to higher logerror)\n",
    "\n",
    "- All in all correlations are very weak (< 0.1). This implies that simple linear models will probably fail to capture the actual relationship of the input to the logerror.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Dimensionality Reduction </h3>\n",
    "\n",
    "Another observation is that we have a rather large number of available features. We would expect some of them to\n",
    "be either correlated to others, or even reduntant alltogether. It might make sense to reduce the dimensions using PCA, and check what percentage of our features are really essential.\n",
    "\n",
    "**Since PCA only handles numerical data we will focus on these only.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# PCA is unsupervised so we should ignore the target variable. It also works with numerical features only\n",
    "def fits_PCA(col, df=merged):\n",
    "    def is_dense(threshold=0.99):\n",
    "        return df[col].isnull().mean() < threshold\n",
    "    def is_numerical():\n",
    "        return df[col].dtype in ['float64', 'int64']\n",
    "    def is_label():\n",
    "        return col in ['logerror', 'ID']\n",
    "    def is_constant():\n",
    "        return df[col].max() == df[col].min()\n",
    "\n",
    "    return is_dense() and is_numerical() and not is_label() and not is_constant()\n",
    "\n",
    "def normalize(df):\n",
    "    return (df - df.mean()) / (df.max() - df.min())\n",
    "\n",
    "# Pick fitting columns for running PCA \n",
    "PCA_cols = [col for col in merged.columns if fits_PCA(col)]\n",
    "\n",
    "# Subset fitting columns, fill missing values and normalize\n",
    "pca_df = normalize(merged[PCA_cols].fillna(merged[PCA_cols].median()))\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(pca_df)\n",
    "plot = sns.tsplot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plot.set(ylabel='% variance explained', xlabel='number of features', title='PCA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> We can safely reduce our features by a factor of 2! </h4>\n",
    "\n",
    "It is obvious from the above figure that we can reach almost 99% of the total variability using only 15 out of 38 numerical features.\n",
    "We can get even greater reduction if we are willing to miss on some information, perhaps using 10 features and retaining more than 95% of the total variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Store necessary data for later analysis </h4>\n",
    "This file so far is the first exprolation of the data. We need to store any usefull data for later analysis. Such as the final merged .csv file and the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged.to_csv('data/merged.csv', index=False)\n",
    "coeffs.to_csv('data/coeffs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's identify categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categorical_var_ID=['HeatingOrSystemTypeID','PropertyLandUseTypeID','StoryTypeID','AirConditioningTypeID','ArchitecturalStyleTypeID','TypeConstructionTypeID','BuildingClassTypeID']\n",
    "categorical_var_Location=['regionidcounty','regionidcity','regionidneighborhood']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
